{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__b9hnQOW0Me",
        "outputId": "7bafa829-b3fa-4677-d950-f2d701acaa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: (0, 0)\n",
            "Up: 0.0\n",
            "Down: 0.0\n",
            "Left: 0.0\n",
            "Right: 0.0\n",
            "\n",
            "State: (0, 1)\n",
            "Up: 0.0\n",
            "Down: 0.0\n",
            "Left: 0.0008100000000000002\n",
            "Right: 0.0\n",
            "\n",
            "State: (0, 2)\n",
            "Up: 0.14332007988258194\n",
            "Down: 0.0\n",
            "Left: 5.2830058699680125\n",
            "Right: 0.0\n",
            "\n",
            "State: (0, 3)\n",
            "Up: 1.5674630322623528\n",
            "Down: 0.8811492877210296\n",
            "Left: 8.198004522953187\n",
            "Right: -0.33941869602293595\n",
            "\n",
            "State: (1, 0)\n",
            "Up: 0.0\n",
            "Down: 0.0\n",
            "Left: 0.0\n",
            "Right: 0.0\n",
            "\n",
            "State: (1, 1)\n",
            "Up: 0.5624395308717249\n",
            "Down: 0.0\n",
            "Left: 0.0\n",
            "Right: 0.0\n",
            "\n",
            "State: (1, 2)\n",
            "Up: -0.16956804177412949\n",
            "Down: -0.15877701030762584\n",
            "Left: -7.290000000000002e-05\n",
            "Right: 2.533795309785333\n",
            "\n",
            "State: (1, 3)\n",
            "Up: -0.19\n",
            "Down: 0.19531958225870524\n",
            "Left: 0.0\n",
            "Right: 0.0\n",
            "\n",
            "State: (2, 0)\n",
            "Up: 1.2072143326596108\n",
            "Down: 0.0\n",
            "Left: 0.0\n",
            "Right: 0.0\n",
            "\n",
            "State: (2, 1)\n",
            "Up: 4.184946618635917\n",
            "Down: 0.0\n",
            "Left: -0.1768246855497998\n",
            "Right: 0.0\n",
            "\n",
            "State: (2, 2)\n",
            "Up: 0.7745932541698167\n",
            "Down: 0.6310549480159503\n",
            "Left: 0.0787228053124741\n",
            "Right: 6.295730172517579\n",
            "\n",
            "State: (2, 3)\n",
            "Up: 0.5792823336656345\n",
            "Down: 5.81638259865239\n",
            "Left: -0.2705940067289908\n",
            "Right: 2.1776615456822306\n",
            "\n",
            "State: (3, 0)\n",
            "Up: 5.958441096776332\n",
            "Down: 1.567643743149398\n",
            "Left: 0.1972954448331632\n",
            "Right: 0.0\n",
            "\n",
            "State: (3, 1)\n",
            "Up: 6.165122127441018\n",
            "Down: 0.048256449882553384\n",
            "Left: 0.6563260083318052\n",
            "Right: 0.4876470957923232\n",
            "\n",
            "State: (3, 2)\n",
            "Up: 7.623742784883031\n",
            "Down: 1.0588735276705439\n",
            "Left: 2.8440678804884176\n",
            "Right: 3.428281736362827\n",
            "\n",
            "State: (3, 3)\n",
            "Up: 7.251536481241166\n",
            "Down: 7.334763383842507\n",
            "Left: 3.651744225401455\n",
            "Right: 6.055660517798637\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title SARSA\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 定义迷宫环境\n",
        "maze = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, -1, 0, -1],\n",
        "    [0, 0, 0, -1],\n",
        "    [-1, 0, 0, 1]\n",
        "])\n",
        "\n",
        "# 定义起始状态和终止状态\n",
        "start_state = (3, 0)\n",
        "goal_state = (3, 3)\n",
        "\n",
        "# 定义动作空间\n",
        "actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]\n",
        "\n",
        "# 初始化状态-动作值函数\n",
        "Q = np.zeros((4, 4, 4))\n",
        "\n",
        "# 定义参数\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "max_episodes = 100\n",
        "\n",
        "# SARSA算法\n",
        "for episode in range(max_episodes):\n",
        "    state = start_state\n",
        "    action = np.random.choice(range(4)) if np.random.rand() < epsilon else np.argmax(Q[state])\n",
        "\n",
        "    while state != goal_state:\n",
        "        # next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "        a = state[0] + actions[action][0]\n",
        "        b = state[1] + actions[action][1]\n",
        "        if a > 3:\n",
        "            a-=1\n",
        "        elif b > 3:\n",
        "            b-=1\n",
        "        elif a < -4:\n",
        "            a+= 1\n",
        "        elif b < -4:\n",
        "            b+= 1\n",
        "        next_state = (a,b)\n",
        "        reward = maze[next_state]\n",
        "        next_action = np.random.choice(range(4)) if np.random.rand() < epsilon else np.argmax(Q[next_state])\n",
        "        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
        "\n",
        "        state = next_state\n",
        "        action = next_action\n",
        "\n",
        "# 输出结果\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        print(\"State:\", (i, j))\n",
        "        print(\"Up:\", Q[i][j][0])\n",
        "        print(\"Down:\", Q[i][j][1])\n",
        "        print(\"Left:\", Q[i][j][2])\n",
        "        print(\"Right:\", Q[i][j][3])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Q-обучение\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = np.zeros((num_states, num_actions))  # 初始化状态-动作值函数为0\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.num_actions)  # 以epsilon的概率随机选择动作\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])  # 否则选择具有最大Q值的动作\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        # 使用Q-learning更新Q值函数\n",
        "        best_next_action = np.argmax(self.Q[next_state])  # 选择下一个状态下具有最大Q值的动作\n",
        "        td_target = reward + self.discount_factor * self.Q[next_state, best_next_action]  # 计算TD目标\n",
        "        td_error = td_target - self.Q[state, action]  # 计算TD误差\n",
        "        self.Q[state, action] += self.learning_rate * td_error  # 更新Q值函数\n",
        "\n",
        "# 创建一个简单的网格世界环境\n",
        "# 在这个环境中，智能体需要找到一个特定位置，并获得+1的奖励，其他位置奖励为0\n",
        "num_states = 5\n",
        "num_actions = 4  # 上、下、左、右四个动作\n",
        "\n",
        "# 初始化Q-learning智能体\n",
        "agent = QLearningAgent(num_states, num_actions)\n",
        "\n",
        "# 运行Q-learning算法进行训练\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(num_states)  # 随机选择一个起始状态\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)  # 根据当前策略选择动作\n",
        "        if action == 0:  # 上\n",
        "            next_state = max(0, state - 1)\n",
        "        elif action == 1:  # 下\n",
        "            next_state = min(num_states - 1, state + 1)\n",
        "        elif action == 2:  # 左\n",
        "            next_state = max(0, state - 1)\n",
        "        else:  # 右\n",
        "            next_state = min(num_states - 1, state + 1)\n",
        "\n",
        "        reward = 0\n",
        "        if next_state == num_states - 1:  # 到达目标位置，获得+1的奖励\n",
        "            reward = 1\n",
        "            done = True\n",
        "\n",
        "        # 更新Q值函数\n",
        "        agent.learn(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state  # 更新状态\n",
        "\n",
        "# 打印学习到的Q值函数\n",
        "print(\"Learned Q-values:\")\n",
        "print(agent.Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWhC_hSZqnc",
        "outputId": "7d2e33d1-3d97-42a2-9486-32afeb019288"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "[[1.43659092 0.36577292 1.59253891 3.64937476]\n",
            " [1.54432707 4.08513067 1.51843521 2.40630573]\n",
            " [2.78177559 3.66249678 2.27370049 4.55156673]\n",
            " [3.284947   3.98496246 3.16216149 5.06385588]\n",
            " [4.51892753 1.17672809 1.12473567 1.73250984]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Двойное Q-обучение\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DoubleQLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.Q1 = np.zeros((num_states, num_actions))  # Q1表\n",
        "        self.Q2 = np.zeros((num_states, num_actions))  # Q2表\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.epsilon:\n",
        "            return np.random.choice(self.num_actions)  # 以epsilon的概率随机选择动作\n",
        "        else:\n",
        "            q_values = self.Q1[state] + self.Q2[state]  # 使用两个Q值函数的平均值来选择动作\n",
        "            return np.argmax(q_values)  # 选择具有最大Q值的动作\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        if np.random.uniform(0, 1) < 0.5:  # 以0.5的概率选择更新哪个Q值函数\n",
        "            best_next_action = np.argmax(self.Q1[next_state])\n",
        "            td_target = reward + self.discount_factor * self.Q2[next_state, best_next_action]\n",
        "            td_error = td_target - self.Q1[state, action]\n",
        "            self.Q1[state, action] += self.learning_rate * td_error\n",
        "        else:\n",
        "            best_next_action = np.argmax(self.Q2[next_state])\n",
        "            td_target = reward + self.discount_factor * self.Q1[next_state, best_next_action]\n",
        "            td_error = td_target - self.Q2[state, action]\n",
        "            self.Q2[state, action] += self.learning_rate * td_error\n",
        "\n",
        "# 创建一个简单的网格世界环境\n",
        "# 在这个环境中，智能体需要找到特定位置，并获得+1的奖励，其他位置奖励为0\n",
        "num_states = 5\n",
        "num_actions = 4  # 上、下、左、右四个动作\n",
        "\n",
        "# 初始化Double Q-learning智能体\n",
        "agent = DoubleQLearningAgent(num_states, num_actions)\n",
        "\n",
        "# 运行Double Q-learning算法进行训练\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(num_states)  # 随机选择一个起始状态\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)  # 根据当前策略选择动作\n",
        "        if action == 0:  # 上\n",
        "            next_state = max(0, state - 1)\n",
        "        elif action == 1:  # 下\n",
        "            next_state = min(num_states - 1, state + 1)\n",
        "        elif action == 2:  # 左\n",
        "            next_state = max(0, state - 1)\n",
        "        else:  # 右\n",
        "            next_state = min(num_states - 1, state + 1)\n",
        "\n",
        "        reward = 0\n",
        "        if next_state == num_states - 1:  # 到达目标位置，获得+1的奖励\n",
        "            reward = 1\n",
        "            done = True\n",
        "\n",
        "        # 更新Q值函数\n",
        "        agent.learn(state, action, reward, next_state)\n",
        "\n",
        "        state = next_state  # 更新状态\n",
        "\n",
        "# 打印学习到的Q值函数\n",
        "print(\"Learned Q-values:\")\n",
        "print(\"Q1:\")\n",
        "print(agent.Q1)\n",
        "print(\"Q2:\")\n",
        "print(agent.Q2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My708PZ7cQvE",
        "outputId": "85e2ec7a-c6a6-4e52-8e16-48b09f7b8447"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "Q1:\n",
            "[[0.43499351 0.         0.32911716 3.0301276 ]\n",
            " [1.48585265 1.55812233 0.82279627 3.46324513]\n",
            " [1.07178318 3.91453899 1.43966066 1.96652781]\n",
            " [1.22365307 3.24724382 1.86128169 4.37919984]\n",
            " [3.79371819 0.57276659 0.61031959 0.59604726]]\n",
            "Q2:\n",
            "[[0.11177675 0.19437398 0.         2.95097101]\n",
            " [0.51674766 0.68537353 0.74017839 3.46560802]\n",
            " [0.9801829  3.89999628 1.67943174 0.86921794]\n",
            " [1.75241339 2.73605752 2.14560506 4.40124908]\n",
            " [3.81455617 0.9422444  0.2509109  0.        ]]\n"
          ]
        }
      ]
    }
  ]
}