{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 定义状态转移矩阵和奖励函数矩阵\n",
        "# 这里使用一个简单的示例，表示一个3个状态、2个动作的MDP问题\n",
        "# 状态转移矩阵为3x2x3的数组，表示每个状态和动作对应的下一个状态的概率\n",
        "# 奖励函数矩阵为3x2的数组，表示每个状态和动作对应的即时奖励\n",
        "transition_probs = np.array([[[0.8, 0.2, 0.0],  # 当前状态为0，执行动作0，下一个状态为0的概率为0.8，为1的概率为0.2\n",
        "                              [0.0, 0.5, 0.5]], # 当前状态为0，执行动作1，下一个状态为1的概率为0.5，为2的概率为0.5\n",
        "                             [[0.7, 0.3, 0.0],  # 当前状态为1，执行动作0，下一个状态为0的概率为0.7，为1的概率为0.3\n",
        "                              [0.0, 0.0, 1.0]], # 当前状态为1，执行动作1，下一个状态为2的概率为1.0\n",
        "                             [[0.0, 1.0, 0.0],  # 当前状态为2，执行动作0，下一个状态为1的概率为1.0\n",
        "                              [0.0, 0.0, 1.0]]])# 当前状态为2，执行动作1，下一个状态为2的概率为1.0\n",
        "rewards = np.array([[5, 0],  # 当前状态为0，执行动作0，即时奖励为5\n",
        "                    [0, 0],  # 当前状态为0，执行动作1，即时奖励为0\n",
        "                    [0, 10]])# 当前状态为1，执行动作0，即时奖励为0；当前状态为1，执行动作1，即时奖励为10\n",
        "\n",
        "def policy_evaluation(transition_probs, rewards, policy, discount_factor=1.0, theta=0.00001, max_iterations=20):\n",
        "    \"\"\"\n",
        "    迭代策略评估算法\n",
        "    \"\"\"\n",
        "    num_states, num_actions, _ = np.shape(transition_probs)\n",
        "    V = np.zeros(num_states)  # 初始化状态值函数为0\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = 0\n",
        "            for a in range(num_actions):\n",
        "                for next_s in range(num_states):\n",
        "                    v += policy[s][a] * transition_probs[s][a][next_s] * (rewards[s][a] + discount_factor * V[next_s])\n",
        "            delta = max(delta, np.abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# 定义初始策略\n",
        "# 这里简单地假设每个状态下都采取相同的策略，即均匀随机选择动作\n",
        "num_states, num_actions, _ = np.shape(transition_probs)\n",
        "policy = np.ones([num_states, num_actions]) / num_actions\n",
        "\n",
        "# 运行迭代策略评估算法，输出每次迭代后的状态值函数\n",
        "for i in range(1, 21):\n",
        "    V = policy_evaluation(transition_probs, rewards, policy, max_iterations=i)\n",
        "    print(f\"Iteration {i}:\")\n",
        "    print(\"Value Function:\", V)\n",
        "    print(\"----------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnYQfFFu5CPd",
        "outputId": "0ce539f0-2b95-40b8-e05f-e2953e9721e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1:\n",
            "Value Function: [2.5    0.875  5.4375]\n",
            "----------------------\n",
            "Iteration 2:\n",
            "Value Function: [ 5.165625    4.65796875 10.04773438]\n",
            "----------------------\n",
            "Iteration 3:\n",
            "Value Function: [ 8.70847266  8.77052793 14.40913115]\n",
            "----------------------\n",
            "Iteration 4:\n",
            "Value Function: [12.65535663 12.94951958 18.67932537]\n",
            "----------------------\n",
            "Iteration 5:\n",
            "Value Function: [16.76430585 17.14959767 22.91446152]\n",
            "----------------------\n",
            "Iteration 6:\n",
            "Value Function: [20.9366969  21.35751433 27.13598792]\n",
            "----------------------\n",
            "Iteration 7:\n",
            "Value Function: [25.13380576 25.56845312 31.35222052]\n",
            "----------------------\n",
            "Iteration 8:\n",
            "Value Function: [29.34053603 29.78056584 35.56639318]\n",
            "----------------------\n",
            "Iteration 9:\n",
            "Value Function: [33.55101075 33.99313523 39.77976421]\n",
            "----------------------\n",
            "Iteration 10:\n",
            "Value Function: [37.76294268 38.20588233 43.99282327]\n",
            "----------------------\n",
            "Iteration 11:\n",
            "Value Function: [41.9754417  42.41869858 48.20576092]\n",
            "----------------------\n",
            "Iteration 12:\n",
            "Value Function: [46.18816141 46.63154174 52.41865133]\n",
            "----------------------\n",
            "Iteration 13:\n",
            "Value Function: [50.40096701 50.84439538 56.63152336]\n",
            "----------------------\n",
            "Iteration 14:\n",
            "Value Function: [54.61380603 55.05725309 60.84438822]\n",
            "----------------------\n",
            "Iteration 15:\n",
            "Value Function: [58.82665805 59.27011239 65.05725031]\n",
            "----------------------\n",
            "Iteration 16:\n",
            "Value Function: [63.03951513 63.48297231 69.27011131]\n",
            "----------------------\n",
            "Iteration 17:\n",
            "Value Function: [67.25237419 67.69583247 73.48297189]\n",
            "----------------------\n",
            "Iteration 18:\n",
            "Value Function: [71.46523401 71.90869272 77.6958323 ]\n",
            "----------------------\n",
            "Iteration 19:\n",
            "Value Function: [75.67809413 76.12155301 81.90869266]\n",
            "----------------------\n",
            "Iteration 20:\n",
            "Value Function: [79.89095437 80.33441331 86.12155298]\n",
            "----------------------\n"
          ]
        }
      ]
    }
  ]
}